import logging
from typing import Callable

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import (RunnableLambda, RunnableMap,
                                       RunnablePassthrough)


def setup_rag_chain(llm: ChatOpenAI, retriever: Callable, template: str) -> RunnableLambda:
    """
    Sets up the Retrieval Augmented Generation (RAG) chain.

    Args:
        llm (ChatOpenAI): OpenAI Model object.
        retriever (Chroma): Chroma vector store.
        template (str): Template string for structuring responses.

    Returns:
        RunnableLambda: Configured RAG chain.
    """
    try:
        # Initialize retriever, prompt template, model and chain for RAG
        prompt = ChatPromptTemplate.from_template(template)
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        return chain
    except Exception as e:
        # Log any exceptions during RAG chain setup
        logging.error(f'Error setting up RAG chain: {e}')
        return None
    

def invoke_rag_chain(chain: RunnableMap, question: str) -> str:
    """
    Invokes the RAG chain with a question.

    Args:
        chain: The RAG chain.
        question (str): The question to be answered.

    Returns:
        str: The answer generated by the RAG chain.
    """
    try:
        # Invoke the RAG chain with the specified question
        return chain.invoke(question)
    except Exception as e:
        # Log any exceptions during the invocation of the RAG chain
        logging.error(f'Error invoking RAG chain: {e}')
        return "An error occurred while generating the response."
